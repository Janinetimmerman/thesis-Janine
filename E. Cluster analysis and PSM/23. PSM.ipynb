{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as sst\n",
    "import os\n",
    "import scipy.stats as sst\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.split(os.getcwd())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = gpd.read_file(parent_dir + '\\\\Data\\\\New\\\\lms_zone_du_new.shp') # LMS Zone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demography\n",
    "demo = pd.read_csv((parent_dir + '\\\\Data\\\\New\\\\zone_demographics.csv'), index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovin = pd.read_csv(parent_dir + '\\\\Data\\\\New\\\\Ovin_final.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modal split travel behaviour\n",
    "ovin_tb = pd.read_csv(parent_dir + '\\\\Data\\\\New\\\\lms_zone_ovin_travel_behaviour_newF.csv', index_col=0)\n",
    "lms_tb = pd.read_csv(parent_dir + '\\\\Data\\\\New\\\\lms_zone_lms_modal_split.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lms_tb2 = lms_tb.iloc[:, 1:8].copy()\n",
    "lms_tb2.iloc[:, 3] = lms_tb2.iloc[:, 3:5].sum(axis=1)\n",
    "lms_tb2 = lms_tb2.drop(columns='Tram/Metro_o')\n",
    "\n",
    "lms_orig = pd.read_csv(parent_dir + '\\\\Data\\\\New\\\\lms_modal_split_orig_abs.csv', index_col=0)\n",
    "lms_tot = lms_orig.iloc[:, 1:8].sum(axis=1) # Total trips for each zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_advanced = np.load(parent_dir + '\\\\Data\\\\New\\\\cluster_labels_advanced.npy')\n",
    "labels_simple = np.load(parent_dir + '\\\\Data\\\\New\\\\cluster_labels_simple.npy')\n",
    "labels_du = np.array(zones.deg_urba - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels_du # Select which cluster set to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6 # And corresponding no. of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovin.FactorV_final.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sort_simple = np.array([5, 6, 3, 0, 2, 4, 1])\n",
    "x_sort_advanced = np.array([0, 4, 3, 6, 5, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.zeros((7, 7))\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in x_sort_advanced:\n",
    "    l = 0\n",
    "\n",
    "    for j in x_sort_simple:\n",
    "\n",
    "        x_i = labels_simple[(labels_advanced == i) & (labels_simple == j)]\n",
    "\n",
    "        relative_size = len(x_i) / len(labels_advanced[labels_simple == j])\n",
    "        # relative_size = len(x_i) \n",
    "\n",
    "\n",
    "        heatmap[k, l] = relative_size\n",
    "\n",
    "        l += 1\n",
    "    \n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(heatmap, cmap='YlOrRd', origin='lower')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Relative share of zones from each unweighted cluster \\nbelonging to a weighted cluster')\n",
    "plt.xticks(np.arange(7), labels=x_sort_simple)\n",
    "plt.yticks(np.arange(7), labels=x_sort_advanced)\n",
    "plt.xlabel('Unweighted clusters')\n",
    "plt.ylabel('Weighted clusters')\n",
    "plt.title('Overlap weighted and unweighted cluster sets');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(n, labels_cluster, x_sort):\n",
    "\n",
    "    heatmap_relative = np.zeros((6, n))\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        for j in range(n):\n",
    "\n",
    "            cluster_zones = zones[labels_cluster == x_sort[j]]\n",
    "            size = len(cluster_zones[cluster_zones.deg_urba == i])\n",
    "            heatmap_relative[i - 1, j] = size / len(zones[zones.deg_urba == i])\n",
    "\n",
    "    plt.imshow(heatmap_relative, cmap='YlOrRd', origin='lower')\n",
    "    cb = plt.colorbar()\n",
    "    cb.set_label('Relative share of zones from each DU in each new cluster')\n",
    "    plt.xticks(np.arange(n), labels=x_sort)\n",
    "    plt.yticks(np.arange(6), labels=np.arange(1, 7))\n",
    "    plt.xlabel('New clusters')\n",
    "    plt.ylabel('Degree of urbanisation')\n",
    "    plt.title('Overlap degree of urbanisation and new clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(features, method='scale'):\n",
    "    \"\"\"\n",
    "    Scale the data to 0-1 or normalize the data\n",
    "\n",
    "    Parameters:\n",
    "    features: numpy array containing all the features\n",
    "    method: the type of data-scaling, string\n",
    "\n",
    "    Returns:\n",
    "    The transformed data as a numpy array\n",
    "    \"\"\"\n",
    "    if method == 'scale':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        return 'Not a valid scaler'\n",
    "    \n",
    "    return scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_df(df, weight_col):\n",
    "    \"\"\"expand the dataframe to prepare for resampling\n",
    "    result is 1 row per count per sample\"\"\"\n",
    "    df = df.reindex(df.index.repeat(df[weight_col]))\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select OViN trips based on departure zone and prepare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ovin(n, labels_cluster):\n",
    "\n",
    "    ovin_trips = []\n",
    "    sizes = np.zeros(n)\n",
    "\n",
    "    for c in range(n):\n",
    "        zone_list = ovin_tb[labels_cluster == c].ZONE_ID\n",
    "        ovin_trips.append(ovin[ovin.VertZone.isin(zone_list)])\n",
    "        sizes[c] = len(ovin_trips[c])\n",
    "\n",
    "    return ovin_trips, sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ovin_cat(n, ovin_trips):\n",
    "\n",
    "    # Get demographic data from OViN\n",
    "    new_trips = []\n",
    "    sizes = np.zeros(n)\n",
    "\n",
    "   \n",
    "    for i in range(n):\n",
    "        # Get useful columns\n",
    "        trips = ovin_trips[i][['Leeftijd', 'Geslacht', 'HHGestInkG', 'HHPers', 'HHSam', \n",
    "                       'MaatsPart', 'Opleiding', 'HHAuto', 'Rijbewijs', 'OVStKaart', 'KHvm', 'FactorV_final']]\n",
    "        \n",
    "        # Make variables dummies or continious\n",
    "        new_trips.append(pd.DataFrame(trips['Leeftijd']))\n",
    "\n",
    "        new_trips[i]['Gender'] = trips['Geslacht'] - 1\n",
    "\n",
    "        new_trips[i]['Income'] = trips['HHGestInkG']\n",
    "        new_trips[i].loc[new_trips[i]['Income'] == 11, 'Income'] = np.nan\n",
    "\n",
    "        new_trips[i]['HH_size'] = trips['HHPers']\n",
    "\n",
    "        new_trips[i]['1person_hh'] = np.zeros(len(trips))\n",
    "        new_trips[i]['2+person_hh'] = np.zeros(len(trips))\n",
    "        new_trips[i]['1parent_hh'] = np.zeros(len(trips))\n",
    "        # new_trips[i]['2parent_hh'] = np.zeros(len(trips)) ## Reference\n",
    "\n",
    "        new_trips[i].loc[trips.HHSam.isin([1]), '1person_hh'] = 1\n",
    "        new_trips[i].loc[trips.HHSam.isin([2, 5, 8]), '2+person_hh'] = 1\n",
    "        new_trips[i].loc[trips.HHSam.isin([6, 7]), '1parent_hh'] = 1\n",
    "        # new_trips[i].loc[trips.HHSam.isin([3, 4]), '2parent_hh'] = 1 ## reference\n",
    "\n",
    "        new_trips[i]['Part_time'] = np.zeros(len(trips))\n",
    "        new_trips[i]['Full_time'] = np.zeros(len(trips))\n",
    "        new_trips[i]['Student'] = np.zeros(len(trips))\n",
    "        # new_trips[i]['Other_part'] = np.zeros(len(trips)) ## Reference\n",
    "\n",
    "        new_trips[i].loc[trips.MaatsPart.isin([1]), 'Part_time'] = 1\n",
    "        new_trips[i].loc[trips.MaatsPart.isin([2]), 'Full_time'] = 1\n",
    "        new_trips[i].loc[trips.MaatsPart.isin([4]), 'Student'] = 1\n",
    "        # new_trips[i].loc[trips.MaatsPart.isin([3, 5, 6, 7, 8, 10]), 'Other_part'] = 1 ## reference\n",
    "        new_trips[i].loc[trips.MaatsPart.isin([9]), 'Part_time'] = np.nan # To filter unknown values\n",
    "\n",
    "        new_trips[i]['Primary_or_less'] = np.zeros(len(trips))\n",
    "        new_trips[i]['lbo_vmbo'] = np.zeros(len(trips))\n",
    "        new_trips[i]['mbo_havo_vwo'] = np.zeros(len(trips))\n",
    "        # new_trips[i]['hbo_wo'] = np.zeros(len(trips)) ## Use as reference\n",
    "        new_trips[i]['Other_edu'] = np.zeros(len(trips))\n",
    "        new_trips[i]['Younger_than_15'] = np.zeros(len(trips))\n",
    "\n",
    "\n",
    "        new_trips[i].loc[trips.Opleiding.isin([0, 1]), 'Primary_or_less'] = 1\n",
    "        new_trips[i].loc[trips.Opleiding.isin([2]), 'lbo_vmbo'] = 1\n",
    "        new_trips[i].loc[trips.Opleiding.isin([3]), 'mbo_havo_vwo'] = 1\n",
    "        # new_trips[i].loc[trips.Opleiding.isin([4]), 'hbo_wo'] = 1 ## reference\n",
    "        new_trips[i].loc[trips.Opleiding.isin([5]), 'Other_edu'] = 1 \n",
    "        new_trips[i].loc[trips.Opleiding.isin([7]), 'Younger_than_15'] = 1 \n",
    "        new_trips[i].loc[trips.Opleiding.isin([6]), 'Other_edu'] = np.nan # To filter unknown values\n",
    "\n",
    "        new_trips[i]['HH_cars'] = trips['HHAuto']\n",
    "        new_trips[i].loc[trips['HHAuto'] == 10, 'HH_cars'] = np.nan\n",
    "\n",
    "        new_trips[i]['Drivers_licence'] = np.zeros(len(trips))\n",
    "        new_trips[i].loc[trips.Rijbewijs.isin([1]), 'Drivers_licence'] = 1\n",
    "        new_trips[i].loc[trips.Rijbewijs.isin([2]), 'Drivers_licence'] = np.nan\n",
    "\n",
    "        new_trips[i]['Student_OV'] = np.zeros(len(trips))\n",
    "        new_trips[i].loc[trips.OVStKaart.isin([1, 2]), 'Student_OV'] = 1\n",
    "        new_trips[i].loc[trips.OVStKaart.isin([3]), 'Student_OV'] = np.nan\n",
    "\n",
    "\n",
    "        # Add variables for modal split\n",
    "        new_trips[i]['Car_driver'] = np.zeros(len(trips))\n",
    "        new_trips[i]['Car_passenger'] = np.zeros(len(trips))\n",
    "        new_trips[i]['Train'] = np.zeros(len(trips))\n",
    "        new_trips[i]['BTM'] = np.zeros(len(trips))\n",
    "        new_trips[i]['Bike'] = np.zeros(len(trips))\n",
    "        new_trips[i]['Walking'] = np.zeros(len(trips))\n",
    "\n",
    "        new_trips[i].loc[trips.KHvm.isin([1]), 'Car_driver'] = 1\n",
    "        new_trips[i].loc[trips.KHvm.isin([2]), 'Car_passenger'] = 1\n",
    "        new_trips[i].loc[trips.KHvm.isin([3]), 'Train'] = 1\n",
    "        new_trips[i].loc[trips.KHvm.isin([4]), 'BTM'] = 1\n",
    "        new_trips[i].loc[trips.KHvm.isin([6]), 'Bike'] = 1\n",
    "        new_trips[i].loc[trips.KHvm.isin([7]), 'Walking'] = 1\n",
    "\n",
    "        new_trips[i].loc[trips.KHvm.isin([5, 8]), 'Walking'] = np.nan # Filter other modes\n",
    "        new_trips[i].loc[trips.KHvm.isnull(), 'Walking'] = np.nan # Filter other modes\n",
    "\n",
    "        new_trips[i]['FactorV'] = trips['FactorV_final']\n",
    "        new_trips[i]['FactorV'] = new_trips[i].FactorV.round().astype(int)\n",
    "        \n",
    "        # Add cluster number\n",
    "        new_trips[i]['Cluster'] = i\n",
    "\n",
    "\n",
    "        new_trips[i] = new_trips[i].dropna()\n",
    "\n",
    "        \n",
    "        sizes[i] = len(new_trips[i])\n",
    "\n",
    "    return new_trips, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips, sizes = get_ovin(n, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trips, n_sizes = get_ovin_cat(n, trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(n_sizes / sizes, 2) # See how much data was deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_arr = np.zeros((n, len(n_trips[0].columns)))\n",
    "\n",
    "for i in range(n):\n",
    "    mean_arr[i] = np.average(n_trips[i], axis=0, weights=n_trips[i].FactorV)\n",
    "\n",
    "mean_df = pd.DataFrame(np.round(mean_arr, 2), columns=n_trips[0].columns).T\n",
    "mean_df\n",
    "\n",
    "# pd.DataFrame(mean_arr, columns=n_trips[0].columns).T\n",
    "# mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First calculate p-values to see if they are statistically different. Do this based on unweighted values for now, to save memory and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p(trips):\n",
    "\n",
    "    p_arr = np.zeros((n, n, 6))\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for m in range(6):\n",
    "                _, p = sst.ttest_ind(trips[i].iloc[:, 18 + m], trips[j].iloc[:, 18 + m])\n",
    "\n",
    "                p_arr[i, j, m] = p\n",
    "\n",
    "    for m in range(6):\n",
    "\n",
    "        df_list.append(np.round(pd.DataFrame(p_arr[:, :, m]), 6))\n",
    "\n",
    "    return p_arr, df_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_arr, df_list = calc_p(n_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travel behaviour in most clusters is statistically different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate prospensity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, do this based on unweighted values to preserve memory. Later check if this works enough. prospensity score is only a means to an end and no end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_clusters(trips):\n",
    "\n",
    "    df_list = []\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "\n",
    "            if (i != j) and (i > j):\n",
    "                \n",
    "                trips[i]['n_cluster'] = 0\n",
    "                trips[j]['n_cluster'] = 1\n",
    "\n",
    "                new_df = pd.concat([trips[i], trips[j]])\n",
    "                new_df = new_df.reset_index()\n",
    "\n",
    "                df_list.append(new_df)\n",
    "                X_list.append(new_df.iloc[:, 1:19])\n",
    "                y_list.append(new_df.iloc[:, -1])\n",
    "    \n",
    "    return df_list, X_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regr(df_list, X_list, y_list):\n",
    "\n",
    "    coeff_df = pd.DataFrame({'Variable':X_list[0].columns})\n",
    "    prob_list = []\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "        x_scaled = scale_data(X_list[i])\n",
    "\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(x_scaled, y_list[i])\n",
    "\n",
    "        coeff_df[f'Cluster:{set(df_list[i][\"Cluster\"])}'] = lr.coef_.ravel()\n",
    "\n",
    "        prob = lr.predict_proba(x_scaled)\n",
    "        df_list[i]['ps'] = prob[:, 1]\n",
    "        prob_list.append(prob)\n",
    "\n",
    "    return coeff_df, df_list, prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ps(n, df_list, plot_size=(3, 7)):\n",
    "\n",
    "    f, ax = plt.subplots(*plot_size)\n",
    "    f.set_figwidth(30)\n",
    "    f.set_figheight(12)\n",
    "\n",
    "    cmap = plt.get_cmap('Dark2', lut=n)\n",
    "\n",
    "    k = 0\n",
    "    for i in range(plot_size[0]):\n",
    "        for j in range(plot_size[1]):\n",
    "            cluster0 = df_list[k][df_list[k]['n_cluster'] == 0].iloc[0].Cluster\n",
    "            cluster1 = df_list[k][df_list[k]['n_cluster'] == 1].iloc[0].Cluster\n",
    "\n",
    "            ax[i, j].hist(df_list[k][df_list[k]['n_cluster'] == 0]['ps'], bins=20, label=f'Cluster {cluster0:.0f}', \n",
    "                            alpha=1)\n",
    "            ax[i, j].hist(df_list[k][df_list[k]['n_cluster'] == 1]['ps'], bins=20, label=f'Cluster {cluster1:.0f}', \n",
    "                            alpha=0.7)\n",
    "            ax[i, j].set_title(f'PS Cluster {cluster0:.0f} and {cluster1:.0f}')\n",
    "            ax[i, j].legend()\n",
    "\n",
    "            if i == plot_size[0] - 1:\n",
    "                ax[i, j].set_xlabel('Propensity score')\n",
    "            \n",
    "            if j == 0:\n",
    "                ax[i, j].set_ylabel('Number of data points')\n",
    "                \n",
    "\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list, X_list, y_list = prepare_clusters(n_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df, df_list, prob_list = logistic_regr(df_list, X_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ps(n, df_list, plot_size=(3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be sufficient overlap to find matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following fuction, PSM is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(df_list, n_neigh=20, caliper=0.01):\n",
    "\n",
    "    # Create lists to store results from all iterations\n",
    "    A_max_list = []\n",
    "    A_min_list = []\n",
    "\n",
    "    match_count_max_list = []\n",
    "    match_count_list = []\n",
    "    match_id_list = []\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "\n",
    "        # Get cluster id's for largest and smallest cluster\n",
    "        cluster_id_max = np.argmax([len(df_list[i][df_list[i].n_cluster == 0]), len(df_list[i][df_list[i].n_cluster == 1])])\n",
    "        cluster_id_min = np.argmin([len(df_list[i][df_list[i].n_cluster == 0]), len(df_list[i][df_list[i].n_cluster == 1])])\n",
    "\n",
    "        # Get array with essential information\n",
    "        A_max = np.array(df_list[i][df_list[i].n_cluster == cluster_id_max][['index', 'FactorV', 'ps']])\n",
    "        A_min = np.array(df_list[i][df_list[i].n_cluster == cluster_id_min][['index', 'FactorV', 'ps']])\n",
    "        \n",
    "        # Get neighbours for each value in largest cluster\n",
    "        dist, index = spatial.KDTree(A_min[:, 2].reshape(-1, 1)).query(A_max[:, 2].reshape(-1, 1), k=n_neigh, distance_upper_bound=caliper)\n",
    "        index[dist == np.inf] = -99999\n",
    "\n",
    "        # Get variables to store results\n",
    "        match_count_max = A_max[:, 1].astype(int).copy()\n",
    "        match_count = np.zeros(len(A_min))\n",
    "        match_id = dict()\n",
    "\n",
    "        # Loop over all values in largest cluster\n",
    "        for idx in range(len(A_max)):\n",
    "\n",
    "            match_id[idx] = [] # empty list to store matches from this index\n",
    "\n",
    "            # Loop over neighbour indices\n",
    "            for m_idx in index[idx]:\n",
    "\n",
    "                if m_idx >= 0: # Check if index other cluster corresponds to real distance\n",
    "                    \n",
    "                    # Check if not all occurences of m_idx have already been matched\n",
    "                    if match_count[m_idx] < A_min[m_idx, 1]:\n",
    "                        \n",
    "                        # Check if the remaining datapoints from match_count_max can be matched at the same time\n",
    "                        if match_count_max[idx] <= (A_min[m_idx, 1] - match_count[m_idx]):\n",
    "                            \n",
    "                            # Add remaining value of match_count_max to match_count\n",
    "                            match_count[m_idx] += match_count_max[idx]\n",
    "                            \n",
    "                            # Add trip ids to match_id dict\n",
    "                            match_id[idx].extend([m_idx] * match_count_max[idx].astype(int))\n",
    "                            \n",
    "                            match_count_max[idx] = 0\n",
    "\n",
    "                            break\n",
    "                        \n",
    "                        # Else match all remaining values from m_idx and the loop continues\n",
    "                        else:\n",
    "                            match_count_max[idx] -= (A_min[m_idx, 1] - match_count[m_idx])\n",
    "\n",
    "                            match_id[idx].extend([m_idx] * (A_min[m_idx, 1] - match_count[m_idx]).astype(int))\n",
    "\n",
    "                            match_count[m_idx] = A_min[m_idx, 1]\n",
    "                \n",
    "                # Extra check if all idx have already been matched\n",
    "                if match_count_max[idx] == 0:\n",
    "                    break\n",
    "\n",
    "        \n",
    "        A_max_list.append(A_max)\n",
    "        A_min_list.append(A_min)\n",
    "\n",
    "        match_count_max_list.append(match_count_max)\n",
    "        match_count_list.append(match_count)\n",
    "        match_id_list.append(match_id)\n",
    "\n",
    "\n",
    "    return A_max_list, A_min_list, match_count_max_list, match_count_list, match_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_matching_results(df_list, A_max_list, A_min_list, match_count_max_list, match_count_list):\n",
    "    \n",
    "    for i in range(len(df_list)):\n",
    "        match_max = A_max_list[i][:, 1] - match_count_max_list[i] # calculate no. of matches for large cluster\n",
    "\n",
    "        # Add column with match count for each cluster\n",
    "        # This will be the new factor V\n",
    "        df_list[i].loc[df_list[i]['index'].isin(A_max_list[i][:, 0]), 'Matched'] = match_max\n",
    "        df_list[i].loc[df_list[i]['index'].isin(A_min_list[i][:, 0]), 'Matched'] = match_count_list[i]\n",
    "    \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_match_size(df_list, A_max_list, A_min_list, match_count_list, match_count_max_list):\n",
    "\n",
    "    size_array = np.zeros((len(df_list), 5))\n",
    "    index_col = []\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "\n",
    "        matched_cluster_size = match_count_list[i].sum()\n",
    "\n",
    "        large_cluster = A_max_list[i][:, 1].sum()\n",
    "        small_cluster = A_min_list[i][:, 1].sum()\n",
    "\n",
    "        size_array[i, 0] = matched_cluster_size\n",
    "        size_array[i, 1] = np.round(matched_cluster_size / large_cluster  * 100, 2)\n",
    "        size_array[i, 2] = np.round(matched_cluster_size / small_cluster * 100, 2)\n",
    "\n",
    "        min_cluster = df_list[i][df_list[i]['index'] == int(A_min_list[i][0, 0])].Cluster.iloc[0]\n",
    "        max_cluster = df_list[i][df_list[i]['index'] == int(A_max_list[i][0, 0])].Cluster.iloc[0]\n",
    "\n",
    "        min_points = len(match_count_list[i][match_count_list[i] != 0])\n",
    "        max_counts = A_max_list[i][:, 1] - match_count_max_list[i]\n",
    "        \n",
    "        max_points = len(max_counts[max_counts != 0])\n",
    "\n",
    "        size_array[i, 3] = min_points\n",
    "        size_array[i, 4] = max_points\n",
    "\n",
    "        index_col.append(f'Cluster {max_cluster} & {min_cluster}')\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(size_array, columns=['Cluster size', f'% of matches large cluster',\n",
    "                                             f'% of matches small cluster', 'Individual data points small cluster',\n",
    "                                             'Individual data points large cluster'])\n",
    "    \n",
    "    df.index = index_col\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## WARNING! TAKES VERY LONG TO RUN!\n",
    "# for i in tqdm(range(100)):\n",
    "#     results = matching(df_list, n_neigh=20, caliper=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(results, open(parent_dir + '\\\\Data\\\\New\\\\results_clustering_DU.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open(parent_dir + '\\\\Data\\\\New\\\\results_clustering_du.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_max_list, A_min_list, match_count_max_list, match_count_list, match_id_list = results[0], results[1], results[2], results[3], results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = merge_matching_results(df_list, A_max_list, A_min_list, match_count_max_list, match_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_size_df = check_match_size(df_list, A_max_list, A_min_list, match_count_list, match_count_max_list)\n",
    "match_size_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compare the demography to see if the matching worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMD(d1, d2):\n",
    "\n",
    "    s1, s2 = d1.std(), d2.std()\n",
    "\n",
    "    return 100 * (d1.mean() - d2.mean()) / np.sqrt((s1 * s1 + s2 * s2) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo_differences(df_list):\n",
    "\n",
    "    cols = df_list[0].columns[1:19]\n",
    "\n",
    "    df_smd_list = []\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "\n",
    "        cluster0 = df_list[i][df_list[i].n_cluster == 0]\n",
    "        cluster1 = df_list[i][df_list[i].n_cluster == 1]\n",
    "\n",
    "        smd_before = SMD(reindex_df(cluster0, 'FactorV').iloc[:, 1:19], reindex_df(cluster1, 'FactorV').iloc[:, 1:19])\n",
    "        smd_after = SMD(reindex_df(cluster0, 'Matched').iloc[:, 1:19], reindex_df(cluster1, 'Matched').iloc[:, 1:19])\n",
    "\n",
    "        df_smd = pd.DataFrame([smd_before, smd_after]).T\n",
    "        df_smd = df_smd.rename(columns={0:'SMD before', 1:'SMD after'})\n",
    "\n",
    "        df_smd_list.append(df_smd)\n",
    "    \n",
    "    return df_smd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_effect_sizes(df_effect_size_list, df_list, plot_size=(7, 3)):\n",
    "\n",
    "    f, ax = plt.subplots(*plot_size)\n",
    "    f.set_figwidth(15)\n",
    "    f.set_figheight(28)\n",
    "    f.set_figheight(20)\n",
    "\n",
    "\n",
    "    # cmap = plt.get_cmap('Dark2', lut=n)\n",
    "    # colors = ['firebrick', 'darkorange', 'olivedrab', 'deepskyblue', 'blueviolet', 'pink']\n",
    "\n",
    "    demo_labels = \"\"\"A: Age; B: Gender; C: Income; D: Household size; E: 1 person household;\\n\n",
    "F: 2+ person household; G: 1 parent household; H: Part time worker; I: Full time worker;\\n\n",
    "J: Student; K: Primary education or less; L: lbo, vmbo; M: mbo, havo, vwo;\\n\n",
    "N: Other education; O: Younger than 15; P: Number of household cars; Q: Driver's licence;\\n\n",
    "R: Student OV\"\"\"\n",
    "\n",
    "    demo_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R']\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    x = np.arange(len(df_effect_size_list[0]))\n",
    "\n",
    "    for i in range(plot_size[0]):\n",
    "        for j in range(plot_size[1]):\n",
    "\n",
    "            cluster0 = df_list[k][df_list[k]['n_cluster'] == 0].iloc[0].Cluster\n",
    "            cluster1 = df_list[k][df_list[k]['n_cluster'] == 1].iloc[0].Cluster\n",
    "\n",
    "            ax[i, j].bar(x - 0.15, df_effect_size_list[k].iloc[:, 0], width=0.25, label='before', color=\"#0072B2\")\n",
    "            ax[i, j].bar(x + 0.15, df_effect_size_list[k].iloc[:, 1], width=0.25, label='after', color=\"#E69F00\")\n",
    "            ax[i, j].axhline(0, color='black')\n",
    "            ax[i, j].axhline(10, color=\"#009E73\", linestyle='--', linewidth=2, label='|SMD| = 10%')\n",
    "            ax[i, j].axhline(-10, color=\"#009E73\", linestyle='--', linewidth=2)\n",
    "\n",
    "            ax[i, j].set_title(f'Cluster {cluster0:.0f} and {cluster1:.0f}')\n",
    "            ax[i, j].set_title(f'Degree of urbanisation {cluster0 + 1:.0f} and {cluster1 + 1:.0f}')\n",
    "\n",
    "            ax[i, j].set_ylim(-30, 30)\n",
    "\n",
    "            if j == 0:\n",
    "                ax[i, j].set_ylabel('Standard Mean difference [%]')\n",
    "            if j == plot_size[1] - 1:\n",
    "                ax[i, j].legend(loc='upper right')\n",
    "\n",
    "            \n",
    "            ax[i, j].set_xticks(x, demo_letters, fontsize=8)\n",
    "            # if i == plot_size[0] - 1:\n",
    "            #     # ax[i, j].set_xticks(x, demo_labels, rotation=90)\n",
    "            #     ax[i, j].set_xticks(x, demo_letters)\n",
    "            # else:\n",
    "            #     ax[i, j].set_xticks(x, demo_letters)\n",
    "\n",
    "            ax[i, j].grid(axis='x', alpha=0.5)\n",
    "\n",
    "            k += 1\n",
    "\n",
    "    # f.suptitle('Standard mean difference before and after PSM', fontsize=25)\n",
    "    # ax[0, 0].plot[]\n",
    "    # props = dict(boxstyle='round', facecolor='lightcyan', alpha=0.5)\n",
    "    # f.text(0, 1, demo_labels, verticalalignment='top', bbox=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smd_list = get_demo_differences(df_list) ## Takes ~4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effect_sizes(df_smd_list, df_list, plot_size=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATE and OBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ate_obe(df_list):\n",
    "\n",
    "    ate_obe_ratio = np.zeros((len(df_list), 7, 6))\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "\n",
    "        cluster0 = df_list[i][df_list[i].n_cluster == 0]\n",
    "        cluster1 = df_list[i][df_list[i].n_cluster == 1]\n",
    "\n",
    "        matched0 = reindex_df(cluster0, 'Matched').iloc[:, 19:25].mean()\n",
    "        matched1 = reindex_df(cluster1, 'Matched').iloc[:, 19:25].mean()\n",
    "\n",
    "        orig0 = reindex_df(cluster0, 'FactorV').iloc[:, 19:25].mean()\n",
    "        orig1 = reindex_df(cluster1, 'FactorV').iloc[:, 19:25].mean()\n",
    "\n",
    "        ate = matched1 - matched0\n",
    "        obe = orig1 - orig0\n",
    "\n",
    "        ratio = ate / obe\n",
    "\n",
    "        ate_obe_ratio[i, 0] = ate * 100\n",
    "        ate_obe_ratio[i, 1] = obe * 100\n",
    "        ate_obe_ratio[i, 2] = ratio\n",
    "        ate_obe_ratio[i, 3] = matched0\n",
    "        ate_obe_ratio[i, 4] = matched1\n",
    "        ate_obe_ratio[i, 5] = orig0\n",
    "        ate_obe_ratio[i, 6] = orig1\n",
    "\n",
    "\n",
    "    \n",
    "    return ate_obe_ratio     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_obe_ratio = ate_obe(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_obe_results = []\n",
    "\n",
    "for i in range(len(ate_obe_ratio)):\n",
    "\n",
    "    cluster0 = df_list[i][df_list[i].n_cluster == 0].Cluster.iloc[0]\n",
    "    cluster1 = df_list[i][df_list[i].n_cluster == 1].Cluster.iloc[0]\n",
    "\n",
    "    ate_obe_results.append([f'Cluster {cluster0} & {cluster1}', *ate_obe_ratio[i][0], *ate_obe_ratio[i][1], *ate_obe_ratio[i][2],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['Car driver', 'Car passenger', 'Train', 'BTM', 'Bike', 'Walking']\n",
    "col2 = ['ATE', 'OBE', 'ratio']\n",
    "\n",
    "col = ['Clusters']\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(6):\n",
    "        col.append(f'{col1[j]} - {col2[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ate_obe = np.round(pd.DataFrame(ate_obe_results, columns=col), 2)\n",
    "df_ate_obe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ate_obe.iloc[:, 1:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look how each clusters differs from the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 2\n",
    "df_ate_obe[df_ate_obe.Clusters.str.contains(f'{l}')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tb results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_results = []\n",
    "\n",
    "for i in range(len(ate_obe_ratio)):\n",
    "\n",
    "    cluster0 = df_list[i][df_list[i].n_cluster == 0].Cluster.iloc[0]\n",
    "    cluster1 = df_list[i][df_list[i].n_cluster == 1].Cluster.iloc[0]\n",
    "\n",
    "    tb_results.append([f'Cluster {cluster0} & {cluster1}', *ate_obe_ratio[i][3], *ate_obe_ratio[i][4], *ate_obe_ratio[i][5], *ate_obe_ratio[i][6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['Car driver', 'Car passenger', 'Train', 'BTM', 'Bike', 'Walking']\n",
    "col2 = ['M0', 'M1', 'O0', 'O1']\n",
    "\n",
    "col = ['Clusters']\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(6):\n",
    "        col.append(f'{col1[j]} - {col2[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb = pd.DataFrame(tb_results, columns=col)\n",
    "df_tb.iloc[:, 1:] = np.round(df_tb.iloc[:, 1:] * 100, 2)\n",
    "df_tb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, 2, 5, 8, 12, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "\n",
    "cluster0 = df_list[c][df_list[c].n_cluster == 0].Cluster.iloc[0]\n",
    "cluster1 = df_list[c][df_list[c].n_cluster == 1].Cluster.iloc[0]\n",
    "\n",
    "f, ax = plt.subplots(1, 3)\n",
    "f.set_figwidth(12)\n",
    "\n",
    "x = np.arange(6)\n",
    "\n",
    "ax[0].bar(x - 0.18, df_tb.iloc[c, 13:19], width=0.25, label=f'Cluster {cluster0}')\n",
    "ax[0].bar(x + 0.18, df_tb.iloc[c, 19:25], width=0.25, label=f'Cluster {cluster1}')\n",
    "ax[0].set_title(f'Travel behaviour before matching')\n",
    "\n",
    "ax[1].bar(x - 0.18, df_tb.iloc[c, 1:7], width=0.25, label=f'Cluster {cluster0}')\n",
    "ax[1].bar(x + 0.18, df_tb.iloc[c, 7:13], width=0.25, label=f'Cluster {cluster1}')\n",
    "ax[1].set_title(f'Travel behaviour after matching')\n",
    "\n",
    "ax[2].bar(x - 0.18, df_ate_obe.iloc[c, 7:13], width=0.25, label='OBE', color='firebrick')\n",
    "ax[2].bar(x + 0.18, df_ate_obe.iloc[c, 1:7], width=0.25, label='ATE', color='forestgreen')\n",
    "ax[2].set_title('Difference before (OBE) and after (ATE)')\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_xticks(x, col1, rotation=45, ha='right')\n",
    "    if i < 2:\n",
    "        ax[i].set_ylim(0, df_tb.iloc[c, 1:25].max() + 5)\n",
    "        ax[i].set_yticks(np.arange(0, df_tb.iloc[c, 1:25].max() + 5, 5))\n",
    "    ax[i].grid(axis='y')\n",
    "    ax[i].set_axisbelow(True)\n",
    "    ax[i].legend()\n",
    "\n",
    "f.suptitle(f'Travel behaviour cluster {cluster0} and cluster {cluster1}', fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get mean values for demography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo_mean(df_list):\n",
    "\n",
    "    mean_list = []\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "\n",
    "        c0_label = df_list[i][df_list[i].n_cluster == 0].Cluster.iloc[0]\n",
    "        c1_label = df_list[i][df_list[i].n_cluster == 1].Cluster.iloc[0]\n",
    "\n",
    "        cluster0 = df_list[i][df_list[i].n_cluster == 0]\n",
    "        cluster1 = df_list[i][df_list[i].n_cluster == 1]\n",
    "\n",
    "        mean0 = reindex_df(cluster0, 'Matched').iloc[:, 1:19].mean()\n",
    "        mean1 = reindex_df(cluster1, 'Matched').iloc[:, 1:19].mean()\n",
    "\n",
    "        df_mean = pd.DataFrame([mean_df.iloc[:18, c0_label], mean_df.iloc[:18, c1_label], np.round(mean0, 2), np.round(mean1, 2)]).T\n",
    "        df_mean = df_mean.rename(columns={c0_label:f'Before: Cluster {c0_label}', c1_label:f'Before: Cluster {c1_label}',\n",
    "                                          'Unnamed 0':f'After: Cluster {c0_label}', 'Unnamed 1':f'After: Cluster {c1_label}'})\n",
    "\n",
    "        mean_list.append(df_mean)\n",
    "    \n",
    "    return mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list = get_demo_mean(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_vals(df_list):\n",
    "\n",
    "    p_before_arr = np.zeros((len(df_list), 6))\n",
    "    p_after_arr = np.zeros((len(df_list), 6))\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "\n",
    "        cluster0 = df_list[i][df_list[i].n_cluster == 0]\n",
    "        cluster1 = df_list[i][df_list[i].n_cluster == 1]\n",
    "\n",
    "        # Get sample size\n",
    "        size_cluster0_before = len(df_list[i][(df_list[i].n_cluster == 0) & (df_list[i].Matched > 0)])\n",
    "        size_cluster0_after = len(df_list[i][(df_list[i].n_cluster == 0) & (df_list[i].Matched > 0)])\n",
    "\n",
    "        size_cluster1_before = len(df_list[i][(df_list[i].n_cluster == 0) & (df_list[i].FactorV > 0)])\n",
    "        size_cluster1_after = len(df_list[i][(df_list[i].n_cluster == 0) & (df_list[i].FactorV > 0)])\n",
    "\n",
    "        # Sample indices\n",
    "        sample0_before = random.sample(list(cluster0.index), k=size_cluster0_before, counts=cluster0.FactorV.astype(int))\n",
    "        sample0_after = random.sample(list(cluster0.index), k=size_cluster0_after, counts=cluster0.Matched.astype(int))\n",
    "\n",
    "        sample1_before = random.sample(list(cluster1.index), k=size_cluster1_before, counts=cluster1.FactorV.astype(int))\n",
    "        sample1_after = random.sample(list(cluster1.index), k=size_cluster1_after, counts=cluster1.Matched.astype(int))\n",
    "    \t\n",
    "        # print(sample1_before)\n",
    "        # Calculate p-values\n",
    "        _, p_before = sst.ttest_ind(cluster0.loc[sample0_before].iloc[:, 19:25], cluster1.loc[sample1_before].iloc[:, 19:25])\n",
    "        _, p_after = sst.ttest_ind(cluster0.loc[sample0_after].iloc[:, 19:25], cluster1.loc[sample1_after].iloc[:, 19:25])\n",
    "\n",
    "\n",
    "        # _, p_before = sst.ttest_ind(reindex_df(cluster0, 'FactorV').iloc[:, 19:25], reindex_df(cluster1, 'FactorV').iloc[:, 19:25])\n",
    "\n",
    "        # _, p_after = sst.ttest_ind(reindex_df(cluster0, 'Matched').iloc[:, 19:25], reindex_df(cluster1, 'Matched').iloc[:, 19:25])\n",
    "\n",
    "        p_before_arr[i] = p_before\n",
    "        p_after_arr[i] = p_after\n",
    "    \n",
    "    return p_before_arr, p_after_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_before_tot_list = []\n",
    "p_after_tot_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    p_before_arr, p_after_arr = p_vals(df_list)\n",
    "\n",
    "    p_before_tot_list.append(p_before_arr)\n",
    "    p_after_tot_list.append(p_after_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_before_succeed = np.zeros(p_after_arr.shape)\n",
    "p_after_succeed = np.zeros(p_after_arr.shape)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    p_before_succeed[p_before_tot_list[i] > 0.05] += 1\n",
    "    p_after_succeed[p_after_tot_list[i] > 0.05] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_results = []\n",
    "\n",
    "for i in range(len(ate_obe_ratio)):\n",
    "\n",
    "    cluster0 = df_list[i][df_list[i].n_cluster == 0].Cluster.iloc[0]\n",
    "    cluster1 = df_list[i][df_list[i].n_cluster == 1].Cluster.iloc[0]\n",
    "\n",
    "    p_results.append([f'Cluster {cluster0} & {cluster1}', *p_after_succeed[i], *p_before_succeed[i]])\n",
    "\n",
    "\n",
    "col1 = ['Car driver', 'Car passenger', 'Train', 'BTM', 'Bike', 'Walking']\n",
    "col2 = ['p-after', 'p_before']\n",
    "\n",
    "col = ['Clusters']\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(6):\n",
    "        col.append(f'{col1[j]} - {col2[i]}')\n",
    "\n",
    "df_p = np.round(pd.DataFrame(p_results, columns=col), 0)\n",
    "df_p.iloc[:, 1:] = df_p.iloc[:, 1:] / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_obe_array = np.array(df_ate_obe.iloc[:, 1:13])\n",
    "ate_obe_array[np.array(df_p.iloc[:, 1:]) >= 5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao = df_ate_obe.copy()\n",
    "df_ao.iloc[:, 1:13] = ate_obe_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = ate_obe_array[:, :6] / ate_obe_array[:, 6:]\n",
    "df_ao.iloc[:, 13:] = np.round(ratio, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ate_obe.to_csv(parent_dir + '\\\\Data\\\\New\\\\ate_obe_DU.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.to_csv(parent_dir + '\\\\Data\\\\New\\\\p_DU.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao.to_csv(parent_dir + '\\\\Data\\\\New\\\\ate_obe_DU_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb.to_csv(parent_dir + '\\\\Data\\\\New\\\\modalsplit_matched_DU.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "geospatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
